{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing selection strategies for EvoGFuzz\n",
    "\n",
    "We use this notebook (work in progress) to test our selection strategies and compare them to the default tournament selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import everything neccessary for our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import trapz\n",
    "import pandas as pd\n",
    "from evogfuzz.evogfuzz_class import Strategy\n",
    "from evogfuzz.evogfuzz_class import EvoGFuzz\n",
    "from evogfuzz.input import Input\n",
    "from typing import List\n",
    "\n",
    "from debugging_framework.benchmark.program import BenchmarkProgram\n",
    "from debugging_framework.benchmark.repository import BenchmarkRepository\n",
    "from debugging_benchmark.calculator.calculator import CalculatorBenchmarkRepository\n",
    "from debugging_benchmark.middle.middle import MiddleBenchmarkRepository\n",
    "from debugging_benchmark.expression.expression import ExpressionBenchmarkRepository\n",
    "from debugging_benchmark.markup.markup import MarkupBenchmarkRepository\n",
    "from debugging_benchmark.tests4py_benchmark.repository import CookieCutterBenchmarkRepository\n",
    "from debugging_benchmark.tests4py_benchmark.repository import PysnooperBenchmarkRepository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the test suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build all test programs from the `debugging-benchmark` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T15:50:51.362264820Z",
     "start_time": "2024-07-10T15:50:37.234234946Z"
    }
   },
   "outputs": [],
   "source": [
    "repos: List[BenchmarkRepository] = [\n",
    "    PysnooperBenchmarkRepository(),\n",
    "    CookieCutterBenchmarkRepository(),\n",
    "    CalculatorBenchmarkRepository(),\n",
    "    MiddleBenchmarkRepository(),\n",
    "    ExpressionBenchmarkRepository(),\n",
    "    MarkupBenchmarkRepository(),\n",
    "]\n",
    "\n",
    "subjects: List[BenchmarkProgram] = []\n",
    "for repo in repos:\n",
    "    for prog in repo.build():\n",
    "        subjects.append(prog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify, that all programs are built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-10T15:51:04.365407729Z"
    }
   },
   "source": [
    "## Running the Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a helper function `run_benchmark` which run the program once using the given selection strategy for the given amout of iterations. It returns the amount of exception triggering inputs found for each iteration in a numpy array. To prevent potential errors from stopping our (time consuming) benchmark, we catch any exceptions and simply return an array full of -1's to flag the failed run while other runs can keep running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evogfuzz.evogfuzz_class import Strategy\n",
    "def run_benchmark(strategy:Strategy, program, iterations=100):\n",
    "    try:\n",
    "        epp = EvoGFuzz(\n",
    "            grammar=program.get_grammar(),\n",
    "            oracle=program.get_oracle(),\n",
    "            inputs=program.get_passing_inputs(),\n",
    "            iterations=iterations,\n",
    "            strategy=strategy\n",
    "            )\n",
    "        found_exception_inputs = epp.fuzz()\n",
    "        test = epp.get_benchmark()\n",
    "        data_run = [data[1] for data in test]\n",
    "    except:\n",
    "        data_run = np.full(iterations, -1)\n",
    "        print(f\"Error: {strategy} in {program}\")\n",
    "    return data_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of multiprocessing to accelerate our benchmarking proccess. We decided to use `ProcessPool` from `pathos` because if allows us to pass the test programs as a parameter. The optimal amout of nodes will depend on the hardware used. We found 50 nodes to perform well on the `gruenau3` server we used.\n",
    "All selection strategies are tested for the given amount of runs and iterations using the given test program. Once finished, the results are saved as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.pools import _ProcessPool as Pool\n",
    "import logging\n",
    "import sys\n",
    "logging.disable(sys.maxsize)\n",
    "from evogfuzz.evogfuzz_class import Strategy\n",
    "import pandas as pd\n",
    "\n",
    "def benchmark_program(subject, runs:int, iterations:int):\n",
    "    strategies = [Strategy.RANK, Strategy.TOURNAMENT, Strategy.ROULETTE, Strategy.STOCHASTIC_UNIVERSAL_SAMPLING, Strategy.TRUNCATION]\n",
    "    p = Pool(50)\n",
    "    tournament = []\n",
    "    roulette = []\n",
    "    sus = []\n",
    "    rank = []\n",
    "    trunc = []\n",
    "    for _ in range(runs):\n",
    "        tournament.append(p.apply_async(run_benchmark, (Strategy.TOURNAMENT, subject, iterations)))\n",
    "        roulette.append(p.apply_async(run_benchmark, (Strategy.ROULETTE, subject, iterations)))\n",
    "        sus.append(p.apply_async(run_benchmark, (Strategy.STOCHASTIC_UNIVERSAL_SAMPLING, subject, iterations)))\n",
    "        rank.append(p.apply_async(run_benchmark, (Strategy.RANK, subject, iterations)))\n",
    "        trunc.append(p.apply_async(run_benchmark, (Strategy.TRUNCATION, subject, iterations)))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    tournament_res = []\n",
    "    roulette_res = []\n",
    "    sus_res = []\n",
    "    rank_res = []\n",
    "    trunc_res = []\n",
    "    for a in tournament:\n",
    "        tournament_res.append(a.get())\n",
    "    for a in roulette:\n",
    "        roulette_res.append(a.get())\n",
    "    for a in sus:\n",
    "        sus_res.append(a.get())\n",
    "    for a in rank:\n",
    "        rank_res.append(a.get())\n",
    "    for a in trunc:\n",
    "        trunc_res.append(a.get())\n",
    "    filename = f\"passing_inputs_{subject}_{Strategy.TOURNAMENT}.csv\"\n",
    "    df = pd.DataFrame(tournament_res)\n",
    "    df.to_csv(filename)\n",
    "    filename = f\"passing_inputs_{subject}_{Strategy.ROULETTE}.csv\"\n",
    "    df = pd.DataFrame(roulette_res)\n",
    "    df.to_csv(filename)\n",
    "    filename = f\"passing_inputs_{subject}_{Strategy.STOCHASTIC_UNIVERSAL_SAMPLING}.csv\"\n",
    "    df = pd.DataFrame(sus_res)\n",
    "    df.to_csv(filename)\n",
    "    filename = f\"passing_inputs_{subject}_{Strategy.RANK}.csv\"\n",
    "    df = pd.DataFrame(rank_res)\n",
    "    df.to_csv(filename)\n",
    "    filename = f\"passing_inputs_{subject}_{Strategy.TRUNCATION}.csv\"\n",
    "    df = pd.DataFrame(trunc_res)\n",
    "    df.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the data we collected to calculate our performance metrics, test their statistical significance and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mannwhitneyu\n",
    "from numpy import trapz\n",
    "from evogfuzz.evogfuzz_class import Strategy\n",
    "\n",
    "strategies = [Strategy.RANK, Strategy.TOURNAMENT, Strategy.ROULETTE, Strategy.STOCHASTIC_UNIVERSAL_SAMPLING, Strategy.TRUNCATION]\n",
    "for subj in subjects:\n",
    "    file_tournament = f\"{subj}_{Strategy.TOURNAMENT}.csv\"\n",
    "    file_rank = f\"{subj}_{Strategy.RANK}.csv\"\n",
    "    file_roulette = f\"{subj}_{Strategy.ROULETTE}.csv\"\n",
    "    file_sus = f\"{subj}_{Strategy.STOCHASTIC_UNIVERSAL_SAMPLING}.csv\"\n",
    "    file_truncation = f\"{subj}_{Strategy.TRUNCATION}.csv\"\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(file_tournament).to_numpy()\n",
    "    df = np.delete(df, 0, 1)\n",
    "    tournament = df.mean(axis=0)\n",
    "    aoc_tournament = [trapz(y, dx=1) for y in df]\n",
    "    total_tournament = [data[99] for data in df]\n",
    "    tte_tournament = np.zeros(30)\n",
    "    for i in range(30):\n",
    "        for j in range(100):\n",
    "            if df[i][j] > 0:\n",
    "                tte_tournament[i] = j + 1\n",
    "                break\n",
    "            tte_tournament[i] = j + 1\n",
    "    df = pd.read_csv(file_rank).to_numpy()\n",
    "    df = np.delete(df, 0, 1)\n",
    "    rank = df.mean(axis=0)\n",
    "    aoc_rank = [trapz(y, dx=1) for y in df]\n",
    "    total_rank = [data[99] for data in df]\n",
    "    tte_rank = np.zeros(30)\n",
    "    for i in range(30):\n",
    "        for j in range(100):\n",
    "            if df[i][j] > 0:\n",
    "                tte_rank[i] = j + 1\n",
    "                break\n",
    "            tte_rank[i] = j + 1\n",
    "    df = pd.read_csv(file_roulette).to_numpy()\n",
    "    df = np.delete(df, 0, 1)\n",
    "    roulette = df.mean(axis=0)\n",
    "    aoc_roulette = [trapz(y, dx=1) for y in df]\n",
    "    total_roulette = [data[99] for data in df]\n",
    "    tte_roulette = np.zeros(30)\n",
    "    for i in range(30):\n",
    "        for j in range(100):\n",
    "            if df[i][j] > 0:\n",
    "                tte_roulette[i] = j + 1\n",
    "                break\n",
    "            tte_roulette[i] = j + 1\n",
    "    df = pd.read_csv(file_sus).to_numpy()\n",
    "    df = np.delete(df, 0, 1)\n",
    "    sus = df.mean(axis=0)\n",
    "    aoc_sus = [trapz(y, dx=1) for y in df]\n",
    "    total_sus = [data[99] for data in df]\n",
    "    tte_sus = np.zeros(30)\n",
    "    for i in range(30):\n",
    "        for j in range(100):\n",
    "            if df[i][j] > 0:\n",
    "                tte_sus[i] = j + 1\n",
    "                break\n",
    "            tte_sus[i] = j + 1\n",
    "    df = pd.read_csv(file_truncation).to_numpy()\n",
    "    df = np.delete(df, 0, 1)\n",
    "    truncation = df.mean(axis=0)\n",
    "    aoc_truncation = [trapz(y, dx=1) for y in df]\n",
    "    total_truncation = [data[99] for data in df]\n",
    "    tte_truncation = np.zeros(30)\n",
    "    for i in range(30):\n",
    "        for j in range(100):\n",
    "            if df[i][j] > 0:\n",
    "                tte_truncation[i] = j + 1\n",
    "                break\n",
    "            tte_truncation[i] = j + 1\n",
    "    plt.figure(dpi=1200)\n",
    "    plt.plot(tournament, label=\"Tournament\")\n",
    "    plt.plot(rank, label=\"Rank\")\n",
    "    plt.plot(roulette, label=\"Roulette\")\n",
    "    plt.plot(sus, label=\"Stochastic Universal Sampling\")\n",
    "    plt.plot(truncation, label=\"Truncation\")\n",
    "    \n",
    "    plt.title(subj)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Found exception inputs')\n",
    "    plt.legend()\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim(left=0)\n",
    "    plt.savefig(f\"./plots/{subj}.png\", dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Area under curve\")\n",
    "    print(\"Tournament: \", np.mean(aoc_tournament))\n",
    "    print(\"Rank : \", np.mean(aoc_rank), mannwhitneyu(aoc_rank, aoc_tournament, alternative=\"greater\").pvalue)\n",
    "    print(\"Roulette: \", np.mean(aoc_roulette), mannwhitneyu(aoc_roulette, aoc_tournament, alternative=\"greater\").pvalue)\n",
    "    print(\"SUS \", np.mean(aoc_sus), mannwhitneyu(aoc_sus, aoc_tournament, alternative=\"greater\").pvalue)\n",
    "    print(\"Truncation: \", np.mean(aoc_truncation), mannwhitneyu(aoc_truncation, aoc_tournament, alternative=\"greater\").pvalue)\n",
    "    \n",
    "    \n",
    "    print(\"\\nTotal exceptions\")\n",
    "    print(\"Tournament: \", np.mean(total_tournament))\n",
    "    print(\"Rank : \", np.mean(total_rank), mannwhitneyu(total_rank, total_tournament, alternative=\"greater\").pvalue)\n",
    "    print(\"Roulette: \", np.mean(total_roulette), mannwhitneyu(total_roulette, total_tournament, alternative=\"greater\").pvalue)\n",
    "    print(\"SUS \", np.mean(total_sus), mannwhitneyu(total_sus, total_tournament, alternative=\"greater\").pvalue)\n",
    "    print(\"Truncation: \", np.mean(total_truncation), mannwhitneyu(total_truncation, total_tournament, alternative=\"greater\").pvalue)\n",
    "    \n",
    "    print(\"\\nTTE\")\n",
    "    print(\"Tournament: \", np.mean(tte_tournament))\n",
    "    print(\"Rank : \", np.mean(tte_rank), mannwhitneyu(tte_rank, tte_tournament, alternative=\"less\").pvalue)\n",
    "    print(\"Roulette: \", np.mean(tte_roulette), mannwhitneyu(tte_roulette, tte_tournament, alternative=\"less\").pvalue)\n",
    "    print(\"SUS \", np.mean(tte_sus), mannwhitneyu(tte_sus, tte_tournament, alternative=\"less\").pvalue)\n",
    "    print(\"Truncation: \", np.mean(tte_truncation), mannwhitneyu(tte_truncation, tte_tournament, alternative=\"less\").pvalue)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
